{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Dataset Classification\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd  # Importing pandas library for data manipulation and analysis\n",
    "import numpy as np  # Importing numpy library for numerical operations\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV  # Importing functions for data splitting and hyperparameter tuning\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder  # Importing preprocessing tools for scaling and one-hot encoding\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Importing ensemble classifiers\n",
    "from lightgbm import LGBMClassifier  # Importing LightGBM classifier\n",
    "from sklearn.impute import SimpleImputer  # Importing SimpleImputer for handling missing data\n",
    "from sklearn.compose import ColumnTransformer  # Importing ColumnTransformer for column-wise transformations\n",
    "from sklearn.pipeline import Pipeline  # Importing Pipeline for creating a processing pipeline\n",
    "from sklearn.metrics import accuracy_score  # Importing accuracy_score for model evaluation\n",
    "from imblearn.over_sampling import BorderlineSMOTE  # Importing BorderlineSMOTE for handling class imbalance\n",
    "import os  # Importing operating system utilities for file handling\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('Train-set.csv')  # Load the training data from a CSV file\n",
    "test_data = pd.read_csv('Test-set.csv')    # Load the test data from a CSV file\n",
    "\n",
    "# Separate the 'Target' column from the train data\n",
    "y_train = train_data['Target']  # Storing the target labels in 'y_train'\n",
    "train_data.drop('Target', axis=1, inplace=True)  # Removing the target column from the training data\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat([train_data, test_data], axis=0)  # Combining train and test data for combined preprocessing\n",
    "\n",
    "# Feature Engineering: Extract day of the week and create a weekend indicator\n",
    "try:\n",
    "    # Convert 'day' column to datetime\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'])\n",
    "    # Extract day of the week (0-6) and create 'day_of_week' feature\n",
    "    all_data['day_of_week'] = all_data['day'].dt.dayofweek\n",
    "    # Create binary indicator for the weekend (Saturday and Sunday)\n",
    "    all_data['is_weekend'] = all_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    # Drop the original 'day' column\n",
    "    all_data.drop('day', axis=1, inplace=True)\n",
    "except (ValueError, OverflowError, pd._libs.tslibs.np_datetime.OutOfBoundsDatetime):\n",
    "    # Handle errors due to invalid date formats\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'], errors='coerce')\n",
    "    all_data['day_of_week'] = all_data['day'].dt.dayofweek\n",
    "    all_data['is_weekend'] = all_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    all_data.drop('day', axis=1, inplace=True)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns  # Identifying numeric columns\n",
    "categorical_cols = all_data.select_dtypes(include=[object]).columns  # Identifying categorical columns\n",
    "\n",
    "# Create transformers for numeric and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median\n",
    "    ('scaler', RobustScaler())  # Scale features using robust scaling\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical variables using one-hot encoding\n",
    "])\n",
    "\n",
    "# Preprocess the data using the column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),  # Apply numeric transformer to numeric columns\n",
    "    ('cat', categorical_transformer, categorical_cols)  # Apply categorical transformer to categorical columns\n",
    "])\n",
    "\n",
    "X_all_preprocessed = preprocessor.fit_transform(all_data)  # Apply preprocessing to all data\n",
    "\n",
    "# Handle class imbalance using BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)  # Initialize BorderlineSMOTE for oversampling\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_all_preprocessed[:train_data.shape[0]], y_train)\n",
    "# Apply BorderlineSMOTE to balance classes in the training data\n",
    "\n",
    "# Create and train optimized models\n",
    "optimized_rf_model = RandomForestClassifier(n_estimators=150, max_depth=9, random_state=42)\n",
    "# Initialize RandomForestClassifier with optimized hyperparameters\n",
    "optimized_gb_model = GradientBoostingClassifier(n_estimators=160, learning_rate=0.05, max_depth=7, random_state=42)\n",
    "# Initialize GradientBoostingClassifier with optimized hyperparameters\n",
    "optimized_lgbm_model = LGBMClassifier(n_estimators=180, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "# Initialize LGBMClassifier with optimized hyperparameters\n",
    "\n",
    "optimized_rf_model.fit(X_train_resampled, y_train_resampled)  # Train RandomForestClassifier on resampled data\n",
    "optimized_gb_model.fit(X_train_resampled, y_train_resampled)  # Train GradientBoostingClassifier on resampled data\n",
    "optimized_lgbm_model.fit(X_train_resampled, y_train_resampled)  # Train LGBMClassifier on resampled data\n",
    "\n",
    "# Get predictions using optimized models\n",
    "test_predictions_rf = optimized_rf_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "# Predict probabilities for class 1 using RandomForestClassifier\n",
    "test_predictions_gb = optimized_gb_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "# Predict probabilities for class 1 using GradientBoostingClassifier\n",
    "test_predictions_lgbm = optimized_lgbm_model.predict_proba(X_all_preprocessed[train_data.shape[0]:])[:, 1]\n",
    "# Predict probabilities for class 1 using LGBMClassifier\n",
    "\n",
    "# Combine the predictions using weighted averaging\n",
    "ensemble_predictions = (0.4 * test_predictions_rf) + (0.4 * test_predictions_gb) + (0.2 * test_predictions_lgbm)\n",
    "# Weighted average of predictions from all three models\n",
    "threshold = 0.5  # Set the threshold for converting probabilities to binary predictions\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Get the 'id' values from the test_data DataFrame\n",
    "submission_ids = test_data['id']\n",
    "\n",
    "# Create binary predictions based on a threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "binary_predictions = (ensemble_predictions >= threshold).astype(int)\n",
    "\n",
    "# Create the submission DataFrame with 'id' and binary 'Target' values\n",
    "submission_df = pd.DataFrame({'id': submission_ids, 'Target': binary_predictions})\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission_binary.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
