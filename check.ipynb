{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'blue-collar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m train_data[\u001b[39m'\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_data[\u001b[39m'\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap({\u001b[39m'\u001b[39m\u001b[39myes\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m})\n\u001b[1;32m     10\u001b[0m \u001b[39m# Calculate correlations\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m correlations \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39;49mcorr()\n\u001b[1;32m     13\u001b[0m \u001b[39m# Print correlations with the target variable (\"Target\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m target_col_correlation \u001b[39m=\u001b[39m correlations[\u001b[39m'\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:10054\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[0;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[1;32m  10052\u001b[0m cols \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m  10053\u001b[0m idx \u001b[39m=\u001b[39m cols\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m> 10054\u001b[0m mat \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto_numpy(dtype\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m, na_value\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mnan, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m  10056\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpearson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m  10057\u001b[0m     correl \u001b[39m=\u001b[39m libalgos\u001b[39m.\u001b[39mnancorr(mat, minp\u001b[39m=\u001b[39mmin_periods)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:1838\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1837\u001b[0m     dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdtype(dtype)\n\u001b[0;32m-> 1838\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mas_array(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, na_value\u001b[39m=\u001b[39;49mna_value)\n\u001b[1;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m dtype:\n\u001b[1;32m   1840\u001b[0m     result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(result, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:1732\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         arr\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1732\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interleave(dtype\u001b[39m=\u001b[39;49mdtype, na_value\u001b[39m=\u001b[39;49mna_value)\n\u001b[1;32m   1733\u001b[0m     \u001b[39m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m     \u001b[39m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[1;32m   1736\u001b[0m \u001b[39mif\u001b[39;00m na_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:1794\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1793\u001b[0m         arr \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mget_values(dtype)\n\u001b[0;32m-> 1794\u001b[0m     result[rl\u001b[39m.\u001b[39;49mindexer] \u001b[39m=\u001b[39m arr\n\u001b[1;32m   1795\u001b[0m     itemmask[rl\u001b[39m.\u001b[39mindexer] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1797\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m itemmask\u001b[39m.\u001b[39mall():\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'blue-collar'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('Train-set.csv')\n",
    "test_data = pd.read_csv('Test-set.csv')\n",
    "\n",
    "# Convert 'yes' and 'no' in the target column to binary (1 and 0)\n",
    "train_data['Target'] = train_data['Target'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = train_data.corr()\n",
    "\n",
    "# Print correlations with the target variable (\"Target\")\n",
    "target_col_correlation = correlations['Target']\n",
    "target_col_correlation = target_col_correlation.sort_values(ascending=False)\n",
    "\n",
    "# Print the correlation values and info\n",
    "print(\"Correlation of each feature with the target variable ('Target'):\")\n",
    "print(target_col_correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of each numeric feature with the target variable ('Target'):\n",
      "Target        1.000000\n",
      "duration      0.405814\n",
      "previous      0.114603\n",
      "Unnamed: 0    0.096272\n",
      "balance       0.060058\n",
      "age           0.027563\n",
      "id           -0.002035\n",
      "pdays        -0.047454\n",
      "campaign     -0.069128\n",
      "Name: Target, dtype: float64\n",
      "\n",
      "Correlation of each encoded categorical feature with the target variable ('Target'):\n",
      "Target              1.000000\n",
      "poutcome_success    0.310858\n",
      "month_oct           0.100405\n",
      "month_mar           0.099643\n",
      "month_sep           0.099401\n",
      "                      ...   \n",
      "month_may          -0.069294\n",
      "poutcome_unknown   -0.071005\n",
      "job_blue-collar    -0.072247\n",
      "housing_yes        -0.075114\n",
      "contact_unknown    -0.105630\n",
      "Name: Target, Length: 93, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('Train-set.csv')\n",
    "test_data = pd.read_csv('Test-set.csv')\n",
    "\n",
    "# Convert 'yes' and 'no' in the target column to binary (1 and 0)\n",
    "train_data['Target'] = train_data['Target'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Select numeric columns for correlation calculation\n",
    "numeric_columns = train_data.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = train_data[numeric_columns].corr()\n",
    "\n",
    "# Print correlations with the target variable (\"Target\")\n",
    "target_col_correlation = correlations['Target']\n",
    "target_col_correlation = target_col_correlation.sort_values(ascending=False)\n",
    "\n",
    "# Print the correlation values and info\n",
    "print(\"Correlation of each numeric feature with the target variable ('Target'):\")\n",
    "print(target_col_correlation)\n",
    "\n",
    "# Handle categorical variables using one-hot encoding\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "encoded_data = pd.get_dummies(train_data[categorical_columns], drop_first=True)\n",
    "\n",
    "# Calculate correlations for one-hot encoded categorical variables\n",
    "encoded_correlations = pd.concat([encoded_data, train_data['Target']], axis=1).corr()\n",
    "\n",
    "# Print correlations with the target variable (\"Target\")\n",
    "encoded_target_correlation = encoded_correlations['Target']\n",
    "encoded_target_correlation = encoded_target_correlation.sort_values(ascending=False)\n",
    "\n",
    "# Print the correlation values and info\n",
    "print(\"\\nCorrelation of each encoded categorical feature with the target variable ('Target'):\")\n",
    "print(encoded_target_correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/b3ws2sh942n4mms732sp99fr0000gn/T/ipykernel_18235/296200383.py:27: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  all_data['day'] = pd.to_datetime(all_data['day'])\n",
      "/var/folders/8p/b3ws2sh942n4mms732sp99fr0000gn/T/ipykernel_18235/296200383.py:36: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  all_data['day'] = pd.to_datetime(all_data['day'], errors='coerce')\n",
      "/Users/macbookpro/Library/Python/3.9/lib/python/site-packages/sklearn/impute/_base.py:555: UserWarning: Skipping features without any observed values: ['day_of_week']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (78161, 1), indices imply (78161, 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m X_all_preprocessed \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mfit_transform(all_data)\n\u001b[1;32m     64\u001b[0m \u001b[39m# Save the preprocessed and feature-engineered data to a CSV file\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m cleaned_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(X_all_preprocessed, columns\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(numeric_cols) \u001b[39m+\u001b[39;49m \u001b[39mlist\u001b[39;49m(preprocessor\u001b[39m.\u001b[39;49mnamed_transformers_[\u001b[39m'\u001b[39;49m\u001b[39mcat\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mnamed_steps[\u001b[39m'\u001b[39;49m\u001b[39mencoder\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mget_feature_names_out(categorical_cols)))\n\u001b[1;32m     66\u001b[0m cleaned_data\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mcleaned_data.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Handle class imbalance using BorderlineSMOTE\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:798\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    790\u001b[0m         mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    791\u001b[0m             arrays,\n\u001b[1;32m    792\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    795\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    796\u001b[0m         )\n\u001b[1;32m    797\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 798\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    799\u001b[0m             data,\n\u001b[1;32m    800\u001b[0m             index,\n\u001b[1;32m    801\u001b[0m             columns,\n\u001b[1;32m    802\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    803\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    804\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    805\u001b[0m         )\n\u001b[1;32m    806\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    808\u001b[0m         {},\n\u001b[1;32m    809\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    813\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:337\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    333\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[1;32m    334\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 337\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:408\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    406\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[1;32m    407\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[0;32m--> 408\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (78161, 1), indices imply (78161, 71)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('Train-set.csv')\n",
    "test_data = pd.read_csv('Test-set.csv')\n",
    "\n",
    "# Separate the 'Target' column (labels) from the training data\n",
    "y_train = train_data['Target']\n",
    "train_data.drop('Target', axis=1, inplace=True)\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "all_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "# Feature Engineering: Extract day of the week and create a weekend indicator\n",
    "try:\n",
    "    # Convert 'day' column to datetime\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'])\n",
    "    # Extract day of the week (0-6) and create 'day_of_week' feature\n",
    "    all_data['day_of_week'] = all_data['day'].dt.dayofweek\n",
    "    # Create binary indicator for weekend (Saturday and Sunday)\n",
    "    all_data['is_weekend'] = all_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    # Drop the original 'day' column\n",
    "    all_data.drop('day', axis=1, inplace=True)\n",
    "except (ValueError, OverflowError, pd._libs.tslibs.np_datetime.OutOfBoundsDatetime):\n",
    "    # Handle errors due to invalid date formats\n",
    "    all_data['day'] = pd.to_datetime(all_data['day'], errors='coerce')\n",
    "    all_data['day_of_week'] = all_data['day'].dt.dayofweek\n",
    "    all_data['is_weekend'] = all_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    all_data.drop('day', axis=1, inplace=True)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = all_data.select_dtypes(include=[object]).columns\n",
    "\n",
    "# Create transformers for numeric and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocess the data using the column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "X_all_preprocessed = preprocessor.fit_transform(all_data)\n",
    "\n",
    "# Save the preprocessed and feature-engineered data to a CSV file\n",
    "cleaned_data = pd.DataFrame(X_all_preprocessed, columns=list(numeric_cols) + list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_cols)))\n",
    "cleaned_data.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "# Handle class imbalance using BorderlineSMOTE\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_all_preprocessed[:train_data.shape[0]], y_train)\n",
    "\n",
    "# Create and train optimized models\n",
    "optimized_rf_model = RandomForestClassifier(n_estimators=150, max_depth=9, random_state=42)\n",
    "optimized_rf_model.fit(X_train_resampled, y_train_resampled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
